{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89e88df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code for Descriptive Statistics with Direct Download\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sklearn\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e0f5fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Cardio.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCardio.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Cardio.csv'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"Cardio.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b7128",
   "metadata": {},
   "source": [
    "# Summary statistics and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ccc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the total number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking format of the data \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9736249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#table showing summary statistics for numeric columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the age in days, already have the age in years\n",
    "df = df.drop('age',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa130a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying summary statiscics for categorical columns\n",
    "df.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3ba6c",
   "metadata": {},
   "source": [
    "# Visual inspection of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e061064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Age Distribution for visualisation\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['age_years'], bins=30, kde=True, color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Distribution of Age in Years')\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673004ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of blood pressure categories for visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='bp_category', data=df, palette='pastel')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Distribution of Blood Pressure Categories')\n",
    "plt.xlabel('Blood Pressure Category')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Show the plot\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b501f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BMI Distribution Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['bmi'], bins=30, kde=True, color='lightcoral')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Distribution of Body Mass Index (BMI)')\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04034bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual inspection of numeric data for outliers \n",
    "\n",
    "continuous_columns = ['age_years', 'height', 'weight', 'bmi']\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(6, 5))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for col, ax in zip(continuous_columns, axs):\n",
    "    sns.histplot(df[col], bins=30, kde=True, ax=ax)\n",
    "    ax.set_title(f'Histogram of {col}')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6532d413",
   "metadata": {},
   "source": [
    "# Chapter 3: Data Preparation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05560087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicate rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16805f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some plots seem slightly skewed so further visual inspection of outliers \n",
    "\n",
    "interest_var = ['height', 'weight', 'ap_hi', 'ap_lo', 'bmi']\n",
    "axes = df[interest_var].plot(kind='box', subplots=True, grid=True, figsize=(15, 7))\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis='x', labelsize=18)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bebeba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#both visualisations show some skew and several outliers so we will carry out further statistical tests to fix outliers\n",
    "\n",
    "#calculating Z-scores for continuous variables\n",
    "z_scores_df = df[continuous_columns].apply(zscore)\n",
    "\n",
    "# Identify rows where any column has a Z-score greater than 3 or less than -3\n",
    "outliers = np.any((z_scores_df > 3) | (z_scores_df < -3), axis=1)\n",
    "\n",
    "# Display the number of outliers identified for each column\n",
    "outliers_by_column = (z_scores_df > 3) | (z_scores_df < -3)\n",
    "outliers_count = outliers_by_column.sum()\n",
    "outliers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de698ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial check to see if the minimum and maximum value are within the realistic range for the variables with high number of outliers\n",
    "df[['weight', 'age_years', 'height', 'ap_hi', 'ap_lo']].agg(['min', 'max', 'mean'])\n",
    "\n",
    "#some of the heights and weights are unrealistic so we will use the three sigma rule to calculate 3 standard deviations\n",
    "#to get a range in which 99.7% of the data falls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the heights and weights are unrealistic so we will use the three sigma rule to calculate 3 standard deviations\n",
    "# to get a range in which 99.7% of the data falls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4faf259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating a lower and upper bound based on 3 std deviations from the mean\n",
    "\n",
    "# Height\n",
    "mean_height = df['height'].mean()\n",
    "std_height = df['height'].std()\n",
    "\n",
    "height_lower_bound = mean_height - 3 * std_height\n",
    "height_upper_bound = mean_height + 3 * std_height\n",
    "# we will use these values to remove outliers that are more than 3 standard deviations from the mean\n",
    "# this will remove unrealistic heights from the dataset\n",
    "\n",
    "# Weight\n",
    "mean_weight = df['weight'].mean()\n",
    "std_weight = df['weight'].std()\n",
    "# calculating a lower and upper bound based on 3 std deciations from the mean\n",
    "weight_lower_bound = mean_weight - 3 * std_weight\n",
    "weight_upper_bound = mean_weight + 3 * std_weight\n",
    "\n",
    "# Bmi\n",
    "mean_bmi = df['bmi'].mean()\n",
    "std_bmi = df['bmi'].std()\n",
    "# calculating a lower and upper bound based on 3 std deciations from the mean\n",
    "bmi_lower_bound = mean_bmi - 3 * std_bmi\n",
    "bmi_upper_bound = mean_bmi + 3 * std_bmi\n",
    "\n",
    "# Systolic blood pressure \n",
    "mean_ap_hi = df['ap_hi'].mean()\n",
    "std_ap_hi = df['ap_hi'].std()\n",
    "# calculating a lower and upper bound based on 3 std deciations from the mean\n",
    "ap_hi_lower_bound = mean_ap_hi - 3 * std_ap_hi\n",
    "ap_hi_upper_bound = mean_ap_hi + 3 * std_ap_hi\n",
    "\n",
    "# Diastolic blood pressure\n",
    "mean_ap_lo = df['ap_lo'].mean()\n",
    "std_ap_lo = df['ap_lo'].std()\n",
    "# calculating a lower and upper bound based on 3 std deciations from the mean\n",
    "ap_lo_lower_bound = mean_ap_lo - 3 * std_ap_lo\n",
    "ap_lo_upper_bound = mean_ap_lo + 3 * std_ap_lo\n",
    "\n",
    "\n",
    "\n",
    "print(' 99.7% of the height values fall between',  round(height_lower_bound), 'cm and', round(height_upper_bound), 'cm', '\\n',\n",
    "'99.7% of the weight values fall between',  round(weight_lower_bound), 'kg and', round(weight_upper_bound), 'kg', '\\n',\n",
    "'99.7% of the bmi values fall between',  round(bmi_lower_bound), 'and', round(bmi_upper_bound), '\\n',\n",
    "'99.7% of the systolic blood pressure values fall between',  round(ap_hi_lower_bound), 'and', round(ap_hi_upper_bound), '\\n',\n",
    "'99.7% of the diastolic blood pressure values fall between',  round(ap_lo_lower_bound), ' and', round(ap_lo_upper_bound))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67627b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set realistic blood pressure ranges\n",
    "height_lower_limit = 140\n",
    "height_upper_limit = 193\n",
    "bmi_lower_limit = 12\n",
    "bmi_upper_limit = 46\n",
    "systolic_lower_limit = 80\n",
    "systolic_upper_limit = 200\n",
    "diastolic_lower_limit = 40\n",
    "diastolic_upper_limit = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd83d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Filtering out values outwith the 99.7% range to remove unrealistic values and outliers\n",
    "outlier_filter = (\n",
    "    (df['height'] >= height_lower_limit) & (df['height'] <= height_upper_limit) &\n",
    "    (df['ap_hi'] >= systolic_lower_limit) & (df['ap_hi'] <= systolic_upper_limit) &\n",
    "    (df['ap_lo'] >= diastolic_lower_limit) & (df['ap_lo'] <= diastolic_upper_limit) &\n",
    "    (df['bmi'] >= bmi_lower_limit) & (df['bmi'] <= bmi_upper_limit)\n",
    ")\n",
    "\n",
    "# Number of rows that don't meet the domain-specific criteria\n",
    "unrealistic_values_count = len(df) - outlier_filter.sum()\n",
    "\n",
    "unrealistic_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47949c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There due to the amount of outliers and unrealistic data in the dataset\n",
    "# we will filter out outliers using 3 standard deviations from the mean  \n",
    "\n",
    "df_cleaned = df[outlier_filter].copy()\n",
    "\n",
    "# Display shape of the final cleaned datasets to show how many rows were removed\n",
    "print(\"Shape of Original Dataset:\", df.shape, \"\\nShape of Cleaned Dataset:\", df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d709bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa3721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to see how much of the data was removed \n",
    "# just under 3% of the data were removed when removing outliers \n",
    "67634 / 68205 * 100\n",
    "\n",
    "100 - 99.16281797522176\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0347c14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visual inspection of outliers in new dataset compared to pre-cleaned dataset using box plot\n",
    "\n",
    "# Number of continuous columns\n",
    "n = len(continuous_columns)\n",
    "\n",
    "# Create a figure and a 2xN grid of subplots (2 rows for the two DataFrames)\n",
    "fig, axes = plt.subplots(2, n, figsize=(15, 7))\n",
    "\n",
    "# Flatten the array of subplots\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each continuous column and plot\n",
    "for i, col in enumerate(continuous_columns):\n",
    "    df_cleaned[col].plot(kind='box', grid=True, ax=axes[i], title=f'Cleaned - {col}')\n",
    "    df[col].plot(kind='box', grid=True, ax=axes[n + i], title=f'Original - {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorise BMI according to standard categories\n",
    "def categorise_bmi(bmi):\n",
    "    if bmi < 18.5:\n",
    "        return 'Underweight'\n",
    "    elif 18.5 <= bmi < 24.9:\n",
    "        return 'Normal'\n",
    "    elif 25 <= bmi < 29.9:\n",
    "        return 'Overweight'\n",
    "    else:\n",
    "        return 'Obesity'\n",
    "\n",
    "    \n",
    "df_cleaned['bmi_category'] = df_cleaned['bmi'].apply(categorise_bmi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebabc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical variables to integers\n",
    "\n",
    "# Specify the custom order for each column\n",
    "custom_order = [['Underweight', 'Normal', 'Overweight', 'Obesity'],  # for 'bmi_category'\n",
    "                ['Normal', 'Elevated', 'Hypertension Stage 1', 'Hypertension Stage 2']]  # for 'bp_category'\n",
    "\n",
    "# Initialize the encoder with the custom order\n",
    "enc = OrdinalEncoder(categories=custom_order)\n",
    "\n",
    "# Fit and transform the DataFrame columns\n",
    "df_cleaned[['bmi_category_encoded', 'bp_category_encoded']] = enc.fit_transform(df_cleaned[['bmi_category', 'bp_category']])\n",
    "\n",
    "\n",
    "# Show the DataFrame\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9680aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['gender', 'bp_category_encoded', 'cholesterol', 'gluc', 'smoke', 'alco', 'cardio', 'bmi_category_encoded']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[categorical_columns].apply(lambda x:x.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e602a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db052d3e",
   "metadata": {},
   "source": [
    "## Visualising relationships in cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac421cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using scatterplots to visualise relationship between age and bmi in our sample \n",
    "\n",
    "df_cleaned['Presence of Cardiovascular Disease'] = df_cleaned['cardio'].map({0: 'No', 1: 'Yes'})\n",
    "\n",
    "# Plot using the new column for the hue\n",
    "g = sns.scatterplot(x=\"bmi\", y=\"age_years\",\n",
    "              hue=\"Presence of Cardiovascular Disease\",\n",
    "              data=df_cleaned) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16677faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_corr = df_cleaned[continuous_columns].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pearson_corr, annot=True, cmap=\"GnBu\", fmt=\".2f\", linewidths=.5)\n",
    "plt.title(\"Correlation Heatmap of Numeric Variables\")\n",
    "plt.show()\n",
    "pearson_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a heat map\n",
    "corr_matrix = df_cleaned.corr(numeric_only=True)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"GnBu\", fmt=\".2f\", linewidths=.5)\n",
    "plt.title(\"Correlation Heatmap of All Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e5426",
   "metadata": {},
   "source": [
    "# UNSUPERVISED ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2313079f",
   "metadata": {},
   "source": [
    "### Code for K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features for clustering\n",
    "features_for_clustering = ['age_years', 'bmi', 'ap_hi', 'ap_lo']\n",
    "\n",
    "# Standardizing the features\n",
    "df_standardized = (df_cleaned[features_for_clustering] - df_cleaned[features_for_clustering].mean()) / df_cleaned[features_for_clustering].std()\n",
    "\n",
    "# Determining optimal number of clusters (k) using the Elbow Method\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(df_standardized)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the Elbow Method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), inertia, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting K-Means with optimal k\n",
    "kmeans_optimal = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "df_cleaned['cluster_kmeans'] = kmeans_optimal.fit_predict(df_standardized)\n",
    "# Code for Cluster Profiles\n",
    "cluster_profiles = df_cleaned.groupby('cluster_kmeans')[features_for_clustering].mean()\n",
    "print(\"Cluster Profiles (K-Means):\\n\", cluster_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7374fdb9",
   "metadata": {},
   "source": [
    "### Selecting a random subset of the data which should reduce the computational load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc41019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Code for Hierarchical Clustering with a Subset of Data\n",
    "subset_size = 1000  # You can change this to a smaller number based on your requirements\n",
    "subset_indices = np.random.choice(df_standardized.index, size=subset_size, replace=False)\n",
    "df_subset = df_standardized.loc[subset_indices]\n",
    "\n",
    "# Using AgglomerativeClustering on the subset\n",
    "hierarchical_optimal_subset = AgglomerativeClustering(n_clusters=3, linkage='ward', metric='euclidean')\n",
    "df_cleaned_subset = df_cleaned.loc[subset_indices].copy()\n",
    "df_cleaned_subset['cluster_hierarchical'] = hierarchical_optimal_subset.fit_predict(df_subset)\n",
    "\n",
    "# Plotting the dendrogram for the subset\n",
    "linkage_matrix_subset = linkage(df_subset, method='ward', metric='euclidean')\n",
    "plt.figure(figsize=(15, 8))\n",
    "dendrogram(linkage_matrix_subset, truncate_mode='level', p=3)  # Adjust 'p' based on your preference\n",
    "plt.title('Truncated Hierarchical Clustering Dendrogram (Subset)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Assigning Hierarchical Clusters with Subset\n",
    "\n",
    "# Select a random subset of the data (adjust the subset size as needed)\n",
    "subset_size = 1000  # You can change this to a smaller number based on your requirements\n",
    "subset_indices = np.random.choice(df_standardized.index, size=subset_size, replace=False)\n",
    "df_subset = df_standardized.loc[subset_indices]\n",
    "\n",
    "# Using AgglomerativeClustering on the subset\n",
    "hierarchical_optimal_subset = AgglomerativeClustering(n_clusters=3, linkage='ward', metric='euclidean')\n",
    "df_cleaned_subset = df_cleaned.loc[subset_indices].copy()\n",
    "df_cleaned_subset['cluster_hierarchical'] = hierarchical_optimal_subset.fit_predict(df_subset)\n",
    "\n",
    "# Assigning the clusters to the entire dataset based on the optimal clusters from the subset\n",
    "df_cleaned['cluster_hierarchical'] = df_cleaned_subset['cluster_hierarchical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f939fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Hierarchical Cluster Profiles\n",
    "cluster_profiles_hierarchical = df_cleaned.groupby('cluster_hierarchical')[features_for_clustering].mean()\n",
    "print(\"Cluster Profiles (Hierarchical):\\n\", cluster_profiles_hierarchical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0c1d5",
   "metadata": {},
   "source": [
    "#  SUPERVISED ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928e825",
   "metadata": {},
   "source": [
    "### Code for Linear Regression with Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35604bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selecting features and target variable\n",
    "features_for_regression = ['age_years', 'bmi', 'ap_hi', 'ap_lo']\n",
    "target_variable = 'cardio'\n",
    "\n",
    "# Splitting the data into training and testing sets (70/30 split)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(df_cleaned[features_for_regression], df_cleaned[target_variable], test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Initializing and fitting the Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nLinear Regression Evaluation:\")\n",
    "print(\"Mean Squared Error:\", mse_linear)\n",
    "print(\"R-squared:\", r2_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b68aa",
   "metadata": {},
   "source": [
    "### Code for Logistic Regression with Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Selecting features and target variable\n",
    "features_for_logistic = ['age_years', 'bmi', 'ap_hi', 'ap_lo', 'cluster_kmeans']\n",
    "target_variable_logistic = 'cardio'\n",
    "\n",
    "# Splitting the data into training and testing sets (70/30 split)\n",
    "X_train_logistic, X_test_logistic, y_train_logistic, y_test_logistic = train_test_split(df_cleaned[features_for_logistic], df_cleaned[target_variable_logistic], test_size=0.3, random_state=42)\n",
    "\n",
    "# Initializing and fitting the Logistic Regression model\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "logistic_model.fit(X_train_logistic, y_train_logistic)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_logistic = logistic_model.predict(X_test_logistic)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy_logistic = accuracy_score(y_test_logistic, y_pred_logistic)\n",
    "confusion_matrix_logistic = confusion_matrix(y_test_logistic, y_pred_logistic)\n",
    "classification_report_logistic = classification_report(y_test_logistic, y_pred_logistic)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nLogistic Regression Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_logistic)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix_logistic)\n",
    "print(\"Classification Report:\\n\", classification_report_logistic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
